\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
\usepackage{url}
\usepackage{hyperref}
\usepackage{listings}[language=Golang]
\usepackage{GoMono}

\lstdefinelanguage{Golang}%
  {morekeywords=[1]{package,import,func,type,struct,return,defer,panic,%
     recover,select,var,const,iota,},%
   morekeywords=[2]{string,uint,uint8,uint16,uint32,uint64,int,int8,int16,%
     int32,int64,bool,float32,float64,complex64,complex128,byte,rune,uintptr,%
     error,interface},%
   morekeywords=[3]{map,slice,make,new,nil,len,cap,copy,close,true,false,%
     delete,append,real,imag,complex,chan,},%
   morekeywords=[4]{for,break,continue,range,go,goto,switch,case,fallthrough,if,%
     else,default,},%
   morekeywords=[5]{Println,Printf,Error,Print,},%
   sensitive=true,%
   morecomment=[l]{//},%
   morecomment=[s]{/*}{*/},%
   morestring=[b]',%
   morestring=[b]",%
   morestring=[s]{`}{`},%
}


\acmConference[LAFI'2020]{Languages for Inference}{January 21,
2020}{New Orleans, LA, USA}

\setcopyright{none}

\bibliographystyle{ACM-Reference-Format}


\title{Probabilistic Programming around Gaussian Processes}
\author{David Tolpin}
\affiliation{
    \institution{PUB+}
    \country{Israel}
}
\email{david.tolpin@gmail.com}

\begin{abstract}
	We present GoGP, a library for probabilistic programming
	around Gaussian processes. Kernels, beliefs about
	hyperparameters and about observation inputs and outputs are
	programmatically expressed using the same differentiable
	probabilistic programming framework. On one hand, a basic
	usage with maximum-likelihood hyperparameter estimates and
	homoscedastic Gaussian noise requires as little coding as
	with any Gaussian process library. On the other hand,
	imposing prior beliefs on hyperparameters, handling input
	uncertainty, heteroscedastic and non-Gaussian noise, or
	change point detection can be organically added on top of a
	Gaussian process kernel. Just like distributions in
	probabilistic programs, kernels can be flexibly combined
	using general code flow rather than through predefined
	combination operators (usually addition and multiplication).
	Implemented in Go, a general-purpose programming language
	popular for server-side programming, our library is both
	well suited for integration into data processing/machine
	learning pipelines and exploits language features such as
	efficient light-weight computation parallelism for
	computation efficiency.  We demonstrate different uses of
	the library on a variety of case studies inspired by
	applications. 
\end{abstract}

\begin{document}
\maketitle

\section{Gaussian Processes in Applications}

Gaussian processes~\cite{} possess properties that make
them the approach of choice in many applications:
\begin{itemize}
	\item A Gaussian process works with as little or as much
		data as available.
	\item Non-uniformly sampled observations, missing
		observations, and observation noise are handled
		organically.
	\item Libraries for inference with Gaussian processes
		are readily available, both standalone and within
		machine learning frameworks, such that Gaussian
		processes can be included into data processing
		pipelines and combined with other machine learning
		algorithms.
\end{itemize}

Gaussian processes are successfully deployed in many settings,
in particular when 
\begin{itemize}
    \item a stationary kernel is sufficient;
	\item kernel hyperparameters can be fixed \textit{a priori} or
		reliably estimated from observations;
	\item observation noise is constant.
\end{itemize}
Most basic cases are indeed covered by the above assumptions,
and library implementations of Gaussian processes prioritize
scalability with number of observations, rich libraries
of kernel functions, and integration of Gaussian processes with
machine learning pipelines.

There are cases though where the noise varies between
observations, structured informative priors must be imposed on
kernel hyperparameters, observation inputs are uncertain, or
observations are best explained by presence of change
points~\cite{}.

While many of the above cases are addressed in research
literature, their diversity is beyond feasible support by a
library, and the practitioner has to implement advanced Gaussian
process algorithms on case-by-case basis. One, laborious, option
is to implement the algorithms in a general-purpose programming
language, along with probabilistic inference for uncertain
quantities --- hyperparameters, observation inputs, location and
number of change points.  This option may, in principle, result
in an efficient and robust implementation, albeit at a high
implementation cost. Another option is to implement the
algorithms in a probabilistic programming framework~\cite{},
which gains support for probabilistic inference on uncertain
parameters but apparently limits implementation efficiency of
Gaussian process-specific computations.

In this work, we advocate a third approach, which combines
advantages of the other two above. We present an architecture of
a Gaussian process library that provides an efficient
implementation of Gaussian process-related computations shared
by all algorithms, but lets program, virtually without
restrictions, both Gaussian process kernels and probabilistic
models of uncertainty, and combine Gaussian processes and
probabilistic models for inference using algorithms supported by
the framework. Our implementation of probabilistic
programming-enabled Gaussian processes, GoGP, is written in Go
and makes extensive use of modelling and inference capabilities
provided by Infergo~\cite{}, a Go library for differentiable
probabilistic programming. In what follows we first outline GoGP
design  and then illustrate the use of GoGP on a number of case
studies inspired by applications.

\section{GoGP --- library for programmable GP inference}

GoGP exports a type, \lstinline{GP}, implementing computations
involved in any use of a Gaussian process. The type has four
methods:

\begin{itemize}
	\item \lstinline{Absorb} --- adds (absorbs) observations;
	\item \lstinline{Produce} --- predicts out distributions;
	\item \lstlinline{Observe} --- computes log-likelihood of
		kernel hyperparameters given observations;
	\item \lstinline{Gradient} --- returns the gradient of
		log-likelihood by hyperparameters, and optionally
		by observations.
\end{itemize}

Methods \lstinline{Observe} and \lstinline{Gradient} also turn
\lstinline{GP} into an Infergo model with a custom gradient,
suitable for passing to Infergo inference algorithms.


package gp // import "bitbucket.org/dtolpin/gogp/gp"


TYPES

type GP struct {
	NDim                   int       // dimensions
	Simil, Noise           Kernel    // kernel
	ThetaSimil, ThetaNoise []float64 // kernel parameters

	// inputs
	X [][]float64 // inputs, for computing covariances
	Y []float64   // outputs

	// Has unexported fields.
}
    Type GP is the barebone implementation of GP.

func (gp *GP) Absorb(x [][]float64, y []float64) (err error)
    Absorb absorbs observations into the process.

func (gp *GP) Gradient() []float64
    Gradient computes the gradient of the log-likelihood with respect to the
    parameters and the inputs (GPML:5.9):

    ∇L = ½ tr((α α^⊤ - Σ^−1) ∂Σ/∂θ), where α = Σ^-1 y

func (gp *GP) LML() float64
    LML computes log marginal likelihood of the kernel given the absorbed
    observations (GPML:5.8):

    L = −½ log|Σ| − ½ y^⊤ α − n/2 log(2π), where α = Σ^-1 y

func (gp *GP) Observe(x []float64) float64
    Observe computes log marginal likelihood of the parameters given the
    observations. The argument is contatenation of log-transformed
    hyperparameters, inputs, and outputs.

    Optionally, the input can be only log-transformed hyperparameters, and then
    * only hyperparameters are inferred; * inputs must be assigned to fields X,
    Y of gp.

func (gp *GP) Produce(x [][]float64) (
	mu, sigma []float64,
	err error,
)
    Produce computes predictions.

type Kernel interface {
	model.Model
	NTheta() int
}
    Type Kernel is the kernel interface, implemented by both covariance and
    noise kernels.


GoGP

\section{Case studies}

\paragraph{Barebones}

\paragraph{Priors on hyperparameters}

\paragraph{Uncertain observation inputs}

\paragraph{Non-Gaussian noise}

\section{Discussion}

\bibliography{refs}

\end{document}

\documentclass[sigplan,review,10pt,anonymous]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{hyperref}
\usepackage{listings}[language=Golang]
\usepackage{GoMono}

%\linespread{1.0}

\lstdefinelanguage{Golang}%
  {morekeywords=[1]{package,import,func,type,struct,return,defer,panic,%
     recover,select,var,const,iota,},%
   morekeywords=[2]{string,uint,uint8,uint16,uint32,uint64,int,int8,int16,%
     int32,int64,bool,float32,float64,complex64,complex128,byte,rune,uintptr,%
     error,interface},%
   morekeywords=[3]{map,slice,make,new,nil,len,cap,copy,close,true,false,%
     delete,append,real,imag,complex,chan,},%
   morekeywords=[4]{for,break,continue,range,go,goto,switch,case,fallthrough,if,%
     else,default,},%
   morekeywords=[5]{Println,Printf,Error,Print,},%
   sensitive=true,%
   morecomment=[l]{//},%
   morecomment=[s]{/*}{*/},%
   morestring=[b]',%
   morestring=[b]",%
   morestring=[s]{`}{`},%
} 

\lstdefinelanguage{Stan}{
  morekeywords=[1]{functions,data,parameters,transformed,model,generated,quantities,%
    for,in,while,print,if,else,lower,upper,increment_log_prob,T,return,%
    reject,integrate_ode,integrate_ode_bdf,integrate_ode_rk45,target},%
  morekeywords=[2]{int,real,vector,%
    ordered,positive_ordered,simplex,unit_vector,%
    row_vector,matrix,%
    cholesky_factor_corr,cholesky_factor_cov,%
    coor_matrix,cov_matrix,%
    void},%
  morekeywords=[3]{%
    Phi,%
    Phi_approx,%
    abs,%
    acos,%
    acosh,%
    append_col,%
    append_row,%
    asin,%
    asinh,%
    atan,%
    atan2,%
    atanh,%
    bernoulli_cdf,%
    bernoulli_cdf_log,%
    bernoulli_lccdf,%
    bernoulli_lcdf,%
    bernoulli_logit_lpmf,%
    bernoulli_logit_lpmf,%
    bernoulli_lpmf,%
    bernoulli_lpmf,%
    bernoulli_rng,%
    bessel_first_kind,%
    bessel_second_kind,%
    beta_binomial_cdf,%
    beta_binomial_cdf_log,%
    beta_binomial_lccdf,%
    beta_binomial_lcdf,%
    beta_binomial_lpmf,%
    beta_binomial_lpmf,%
    beta_binomial_rng,%
    beta_cdf,%
    beta_cdf_log,%
    beta_lccdf,%
    beta_lcdf,%
    beta_lpdf,%
    beta_lpdf,%
    beta_rng,%
    binary_log_loss,%
    binomial_cdf,%
    binomial_cdf_log,%
    binomial_coefficient_log,%
    binomial_lccdf,%
    binomial_lcdf,%
    binomial_logit_lpmf,%
    binomial_logit_lpmf,%
    binomial_lpmf,%
    binomial_lpmf,%
    binomial_rng,%
    block,%
    categorical_logit_lpmf,%
    categorical_logit_lpmf,%
    categorical_lpmf,%
    categorical_lpmf,%
    categorical_rng,%
    cauchy_cdf,%
    cauchy_cdf_log,%
    cauchy_lccdf,%
    cauchy_lcdf,%
    cauchy_lpdf,%
    cauchy_lpdf,%
    cauchy_rng,%
    cbrt,%
    ceil,%
    chi_square_cdf,%
    chi_square_cdf_log,%
    chi_square_lccdf,%
    chi_square_lcdf,%
    chi_square_lpdf,%
    chi_square_lpdf,%
    chi_square_rng,%
    cholesky_decompose,%
    col,%
    cols,%
    columns_dot_product,%
    columns_dot_self,%
    cos,%
    cosh,%
    crossprod,%
    csr_extract_u,%
    csr_extract_v,%
    csr_extract_w,%
    csr_matrix_times_vector,%
    csr_to_dense_matrix,%
    cumulative_sum,%
    determinant,%
    diag_matrix,%
    diag_post_multiply,%
    diag_pre_multiply,%
    diagonal,%
    digamma,%
    dims,%
    dirichlet_lpdf,%
    dirichlet_lpdf,%
    dirichlet_rng,%
    distance,%
    dot_product,%
    dot_self,%
    double_exponential_cdf,%
    double_exponential_cdf_log,%
    double_exponential_lccdf,%
    double_exponential_lcdf,%
    double_exponential_lpdf,%
    double_exponential_lpdf,%
    double_exponential_rng,%
    e,%
    eigenvalues_sym,%
    eigenvectors_sym,%
    erf,%
    erfc,%
    exp,%
    exp2,%
    exp_mod_normal_cdf,%
    exp_mod_normal_cdf_log,%
    exp_mod_normal_lccdf,%
    exp_mod_normal_lcdf,%
    exp_mod_normal_lpdf,%
    exp_mod_normal_lpdf,%
    exp_mod_normal_rng,%
    expm1,%
    exponential_cdf,%
    exponential_cdf_log,%
    exponential_lccdf,%
    exponential_lcdf,%
    exponential_lpdf,%
    exponential_lpdf,%
    exponential_rng,%
    fabs,%
    falling_factorial,%
    fdim,%
    floor,%
    fma,%
    fmax,%
    fmin,%
    fmod,%
    frechet_cdf,%
    frechet_cdf_log,%
    frechet_lccdf,%
    frechet_lcdf,%
    frechet_lpdf,%
    frechet_lpdf,%
    frechet_rng,%
    gamma_cdf,%
    gamma_cdf_log,%
    gamma_lccdf,%
    gamma_lcdf,%
    gamma_lpdf,%
    gamma_lpdf,%
    gamma_p,%
    gamma_q,%
    gamma_rng,%
    gaussian_dlm_obs_lpdf,%
    gaussian_dlm_obs_lpdf,%
    get_lp,%
    gumbel_cdf,%
    gumbel_cdf_log,%
    gumbel_lccdf,%
    gumbel_lcdf,%
    gumbel_lpdf,%
    gumbel_lpdf,%
    gumbel_rng,%
    head,%
    hypergeometric_lpmf,%
    hypergeometric_lpmf,%
    hypergeometric_rng,%
    hypot,%
    if_else,%
    inc_beta,%
    int_step,%
    inv,%
    inv_chi_square_cdf,%
    inv_chi_square_cdf_log,%
    inv_chi_square_lccdf,%
    inv_chi_square_lcdf,%
    inv_chi_square_lpdf,%
    inv_chi_square_lpdf,%
    inv_chi_square_rng,%
    inv_cloglog,%
    inv_gamma_cdf,%
    inv_gamma_cdf_log,%
    inv_gamma_lccdf,%
    inv_gamma_lcdf,%
    inv_gamma_lpdf,%
    inv_gamma_lpdf,%
    inv_gamma_rng,%
    inv_logit,%
    inv_phi,%
    inv_sqrt,%
    inv_square,%
    inv_wishart_lpdf,%
    inv_wishart_lpdf,%
    inv_wishart_rng,%
    inverse,%
    inverse_spd,%
    is_inf,%
    is_nan,%
    lbeta,%
    lchoose,%
    lgamma,%
    lkj_corr_cholesky_lpdf,%
    lkj_corr_cholesky_lpdf,%
    lkj_corr_cholesky_rng,%
    lkj_corr_lpdf,%
    lkj_corr_lpdf,%
    lkj_corr_rng,%
    lmgamma,%
    lmultiply,%
    log,%
    log10,%
    log1m,%
    log1m_exp,%
    log1m_inv_logit,%
    log1p,%
    log1p_exp,%
    log2,%
    log_determinant,%
    log_diff_exp,%
    log_falling_factorial,%
    log_inv_logit,%
    log_mix,%
    log_rising_factorial,%
    log_softmax,%
    log_sum_exp,%
    logistic_cdf,%
    logistic_cdf_log,%
    logistic_lccdf,%
    logistic_lcdf,%
    logistic_lpdf,%
    logistic_lpdf,%
    logistic_rng,%
    logit,%
    lognormal_cdf,%
    lognormal_cdf_log,%
    lognormal_lccdf,%
    lognormal_lcdf,%
    lognormal_lpdf,%
    lognormal_lpdf,%
    lognormal_rng,%
    machine_precision,%
    max,%
    mdivide_left_tri_low,%
    mdivide_right_tri_low,%
    mean,%
    min,%
    modified_bessel_first_kind,%
    modified_bessel_second_kind,%
    multi_gp_cholesky_lpdf,%
    multi_gp_cholesky_lpdf,%
    multi_gp_lpdf,%
    multi_gp_lpdf,%
    multi_normal_cholesky_lpdf,%
    multi_normal_cholesky_lpdf,%
    multi_normal_cholesky_rng,%
    multi_normal_lpdf,%
    multi_normal_lpdf,%
    multi_normal_prec_lpdf,%
    multi_normal_prec_lpdf,%
    multi_normal_rng,%
    multi_student_t_lpdf,%
    multi_student_t_lpdf,%
    multi_student_t_rng,%
    multinomial_lpmf,%
    multinomial_lpmf,%
    multinomial_rng,%
    multiply_log,%
    multiply_lower_tri_self_transpose,%
    neg_binomial_2_cdf,%
    neg_binomial_2_cdf_log,%
    neg_binomial_2_lccdf,%
    neg_binomial_2_lcdf,%
    neg_binomial_2_log_lpmf,%
    neg_binomial_2_log_lpmf,%
    neg_binomial_2_log_rng,%
    neg_binomial_2_lpmf,%
    neg_binomial_2_lpmf,%
    neg_binomial_2_rng,%
    neg_binomial_cdf,%
    neg_binomial_cdf_log,%
    neg_binomial_lccdf,%
    neg_binomial_lcdf,%
    neg_binomial_lpmf,%
    neg_binomial_lpmf,%
    neg_binomial_rng,%
    negative_infinity,%
    normal_cdf,%
    normal_cdf_log,%
    normal_lccdf,%
    normal_lcdf,%
    normal_lpdf,%
    normal_lpdf,%
    normal_rng,%
    not_a_number,%
    num_elements,%
    ordered_logistic_lpmf,%
    ordered_logistic_lpmf,%
    ordered_logistic_rng,%
    owens_t,%
    pareto_cdf,%
    pareto_cdf_log,%
    pareto_lccdf,%
    pareto_lcdf,%
    pareto_lpdf,%
    pareto_lpdf,%
    pareto_rng,%
    pareto_type_2_cdf,%
    pareto_type_2_cdf_log,%
    pareto_type_2_lccdf,%
    pareto_type_2_lcdf,%
    pareto_type_2_lpdf,%
    pareto_type_2_lpdf,%
    pareto_type_2_rng,%
    pi,%
    poisson_cdf,%
    poisson_cdf_log,%
    poisson_lccdf,%
    poisson_lcdf,%
    poisson_log_lpmf,%
    poisson_log_lpmf,%
    poisson_log_rng,%
    poisson_lpmf,%
    poisson_lpmf,%
    poisson_rng,%
    positive_infinity,%
    pow,%
    prod,%
    qr_Q,%
    qr_R,%
    quad_form,%
    quad_form_diag,%
    quad_form_sym,%
    rank,%
    rayleigh_cdf,%
    rayleigh_cdf_log,%
    rayleigh_lccdf,%
    rayleigh_lcdf,%
    rayleigh_lpdf,%
    rayleigh_lpdf,%
    rayleigh_rng,%
    rep_array,%
    rep_matrix,%
    rep_row_vector,%
    rep_vector,%
    rising_factorial,%
    round,%
    row,%
    rows,%
    rows_dot_product,%
    rows_dot_self,%
    scaled_inv_chi_square_cdf,%
    scaled_inv_chi_square_cdf_log,%
    scaled_inv_chi_square_lccdf,%
    scaled_inv_chi_square_lcdf,%
    scaled_inv_chi_square_lpdf,%
    scaled_inv_chi_square_lpdf,%
    scaled_inv_chi_square_rng,%
    sd,%
    segment,%
    sin,%
    singular_values,%
    sinh,%
    size,%
    skew_normal_cdf,%
    skew_normal_cdf_log,%
    skew_normal_lccdf,%
    skew_normal_lcdf,%
    skew_normal_lpdf,%
    skew_normal_lpdf,%
    skew_normal_rng,%
    softmax,%
    sort_asc,%
    sort_desc,%
    sort_indices_asc,%
    sort_indices_desc,%
    sqrt,%
    sqrt2,%
    square,%
    squared_distance,%
    step,%
    student_t_cdf,%
    student_t_cdf_log,%
    student_t_lccdf,%
    student_t_lcdf,%
    student_t_lpdf,%
    student_t_lpdf,%
    student_t_rng,%
    sub_col,%
    sub_row,%
    sum,%
    tail,%
    tan,%
    tanh,%
    tcrossprod,%
    tgamma,%
    to_array_1d,%
    to_array_2d,%
    to_matrix,%
    to_row_vector,%
    to_vector,%
    trace,%
    trace_gen_quad_form,%
    trace_quad_form,%
    trigamma,%
    trunc,%
    uniform_cdf,%
    uniform_cdf_log,%
    uniform_lccdf,%
    uniform_lcdf,%
    uniform_lpdf,%
    uniform_lpdf,%
    uniform_rng,%
    variance,%
    von_mises_lpdf,%
    von_mises_lpdf,%
    von_mises_rng,%
    weibull_cdf,%
    weibull_cdf_log,%
    weibull_lccdf,%
    weibull_lcdf,%
    weibull_lpdf,%
    weibull_lpdf,%
    weibull_rng,%
    wiener_lpdf,%
    wiener_lpdf,%
    wishart_lpdf,%
    wishart_lpdf,%
    wishart_rng
  },%
  otherkeywords={<-,~,+=,=},%
  sensitive=true,%
  morecomment=[l]{\#},%
  morecomment=[l]{//},%
  morecomment=[n]{/*}{*/},%
  string=[d]"%,
  literate={<-}{{$\leftarrow$}}1 {~}{{$\sim$}}1%
}

\lstset{ % add your own preferences
    frame=single,
    basicstyle=\ttfamily\footnotesize,
    numbers=none,
    numbersep=4pt,
    showstringspaces=false,
    tabsize=4,
    language=Golang % this is it !
}


\acmConference[Onward! 2019]{SPLASH Onward! 2019}{October 20-25, 2019}{Athens, Greece}

% Copyright
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


\bibliographystyle{ACM-Reference-Format}

\title{Deployable Probabilistic Programming}
\author{David Tolpin}
\affiliation{
    \institution{PUB+}
    \country{Israel}
}
\email{david.tolpin@gmail.com}

\begin{abstract}
	We propose design guidelines for a probabilistic programming
	facility suitable for deployment as a part of a production
	software system. As a reference implementation, we introduce
	Infergo, a probabilistic programming facility for Go, a modern
	programming language of choice for server-side software
	development. We argue that a similar probabilistic
	programming facility can be added to most modern general-purpose
	programming languages.

	Probabilistic programming enables automatic tuning of
	program parameters and algorithmic decision making through
	probabilistic inference based on the data. To facilitate
	addition of probabilistic programming capabilities to other
	programming languages, we share design choices and
	implementation techniques employed in development of
	Infergo. We illustrate applicability of Infergo to various
	use cases on case studies, and evaluate Infergo's
	performance on several benchmarks, comparing Infergo to
	dedicated inference-centric probabilistic programming
	systems.
\end{abstract}

\begin{document}
\maketitle

\begin{sloppypar}

\section{Introduction}

	Probabilistic programming \cite{GMR+08,MSP14,WVM14,GS15}
	represents statistical models as programs written in an
	otherwise general programming language that provides syntax
	for the definition and conditioning of random variables.
	Inference can be performed on probabilistic programs to
	obtain the posterior distribution or point estimates of the
	variables. Inference algorithms are provided by the
	probabilistic programming system, and each algorithm is
	usually applicable to a wide class of probabilistic programs
	in a black-box manner. The algorithms include
	Metropolis-Hastings\cite{WSG11,MSP14,YHG14}, Hamiltonian
	Monte Carlo~\cite{Stan17}, expectation
	propagation~\cite{MWG+10}, extensions of Sequential Monte
	Carlo~\cite{WVM14,MYM+15,PWD+14}, variational
	inference~\cite{WW13,KTR+17}, gradient-based
	optimization~\cite{Stan17,BCJ+19}, and others.

	There are two polar views on the role of probabilistic
	programming.  One view is that probabilistic programming is
	a flexible framework for specification and analysis of
	statistical models~\cite{WSG11,TMY+16,GXG18}. Proponents of
	this view perceive probabilistic programming as a tool for
	data science practitioners. A typical workflow consists of
	data acquisition and pre-processing, followed by several
	iterations of exploratory model design and testing of
	inference algorithms. Once a sufficiently robust statistical
	model is obtained, analysis results are post-processed,
	visualized, and occasionally integrated into algorithms of a
	production software system. 

	The other, emerging, view is that probabilistic programming
	is an extension of a regular programming toolbox allowing
	algorithms implemented in general-purpose programming languages to
	have learnable parameters. In this view, statistical models
	are integrated into the software system, and inference and
	optimization take place during data processing, prediction,
	and algorithmic decision making in production.
	Probabilistic programming code runs unattended, and
	inference results are an integral part of the algorithms.

	In this work we are concerned with the later view on
	probabilistic programming. Our objective is to pave a path
	to deployment of probabilistic programs in production
	systems, promoting a wider adoption of probabilistic
	programming as a flexible tool for ubiquitous statistical
	inference. Most probabilistic programming systems are
	suited, to a certain extent, to be used in either the
	exploratory or the production scenario. However, the
	proliferation of probabilistic programming languages and
	systems\footnote{There are 45 probabilistic programming
	languages listed on
	Wikipedia~\cite{wiki:Probabilistic_programming_language}. 18
	probabilistic programming languages were presented during
	the developer meetup at PROBPROG'2018, the inaugural
	conference on probabilistic
	programming,~\url{http://probprog.cc/}, half of which are
	not on the Wikipedia list.} on one hand, and scarcity of success
	stories about production use of probabilistic programming on
	the other hand, suggest that there is a need for new ideas
	and approaches to make probabilistic programming models
	available for production use.

	Our attitude differs from the established approach in that
	instead of proposing yet another probabilistic programming
	language, either genuinely different~\cite{MMR+07,Stan17} or
	derived from an existing general-purpose programming
	language by extending the syntax or changing the
	semantics~\cite{GMR+08,TMY+16,GXG18}, we advocate enabling
	probabilistic inference on models written in a
	general-purpose probabilistic programming language, the same
	one as the language used to program the bulk of the system. 
	We formulate guidelines for such implementation, and based
	on the guidelines, introduce a probabilistic programming
	facility for the Go programming language~\cite{Golang}. By
	explaining our design choices and implementation techniques,
	we argue that a similar facility can be added to any modern
	general-purpose purpose programming language, giving a
	production system modeling and inference capabilities which
	do not fall short from and sometimes exceed those of
	probabilistic programming-centric systems with custom
	languages and runtimes. Availability of such facilities
	in major general-purpose programming languages would free
	probabilistic programming researchers and practitioners
	alike from language wars~\cite{SH14} and foster practical
	applications of probabilistic programming.

	\subsection*{Contributions}

	This work brings the following major contributions:

	\begin{itemize}
		\item Guidelines for design and implementation of a
			probabilistic programming facility deployable as a part 
			of a production software system.
		\item A reference implementation of deployable probabilistic
			programming facility for the Go programming language.
		\item Design and implementation highlights outlining
			important choices we made in implementation of the
			probabilistic programming facility, and providing a
			basis for implementation of a similar facility for
			other general-purpose programming languages.
	\end{itemize}

\section{Related Work}

Our work is related to three interconnected areas of research:
\begin{enumerate}
	\item Design and implementation of probabilistic programming
		languages. 
	\item Integration and interoperability between probabilistic
		programming models and the surrounding software
		systems.
	\item Differentiable programming for machine learning.
\end{enumerate}

Probabilistic programming is traditionally associated with
design and implementation of probabilistic programming
\textit{languages}. An apparent justification for a new
programming language is that a probabilistic program has a
different semantics~\cite{GHNR14} than a `regular' program of
similar structure. \citet{GS15} offer a
systematic approach to design and implementation of
probabilistic programming languages based on extension of the
syntax of a (subset of) general-purpose programming language and
transformational compilation to continuation-passing form. 
\citet{MPY+18} give a comprehensive overview of
available choices of design and implementation of first-order
and higher-order probabilistic programming languages.
Stan~\cite{Stan17} is built around an imperative probabilistic
programming language with program structure tuned for most
efficient execution of inference on the model.
SlicStan~\cite{GGS19} is built on top of Stan as a
source-to-source compiler, providing the model developer with a
more intuitive language while retaining performance benefits of
Stan. Our work differs from existing approaches in that we
advocate enabling probabilistic programming in a general-purpose
programming language by leveraging existing capabilities of the
language, rather than building a new language on top or besides
an existing one.

The need for integration between probabilistic programs and the
surrounding software environment is well understood in the
literature~\cite{TMY+16,BCJ+19}. Probabilistic programming
languages are often implemented as embedded domain-specific
languages~\cite{SGG15,GXG18,TMY+16} and include a mechanism of
calling functions from libraries of the host languages. Our work
goes a step further in this direction: since the language of
implementation of probabilistic models coincides with the host
language, any library or user-defined function of the host
language can be directly called from the probabilistic model,
and model methods can be directly called from the host language.

Automatic differentiation is widely employed in machine
learning~\cite{BPR+17,MBB+18} where it is also known as
`differentiable programming', and is responsible for enabling
efficient inference in many probabilistic programming
systems~\cite{Stan17,GXG18,ISF+18,BCJ+19,THS+17}. Different
automatic differentiation techniques~\cite{GW08} allow 
different compromises between flexibility, efficiency, and
feature-richness~\cite{SDT14,PGC+17,ISF+18}.  Automatic
differentiation is usually implemented through either
operator overloading~\cite{Stan17,PGC+17,GXG18} or source
code transformation~\cite{ISF+18,MMW18}. Our work, too, relies
on automatic differentiation, implemented as source code
transformation. However, a novelty of our approach is that
instead of using explicit calls or directives to denote parts of
code which needs to be differentiated, we rely on the type
system of Go to selectively differentiate the code relevant for
inference, thus combining advantages of both operator
overloading and source code transformation.

\section{Challenges}

Incorporating a probabilistic program, or rather a probabilistic
procedure, within a larger code body appears to be rather
straightforward: one implements the model in the probabilistic
programming language, fetches and preprocesses the data in the
host programming language, passes the data and the model to an
inference algorithm, and post-processes the results in the host
programming language again to make algorithmic decisions based
on inference outcomes. To choose the best option for each
particular application, probabilistic programming languages and
systems are customarily compared by the expressive power
(classes of probabilistic models that can be represented),
conciseness (e.g.  the number of lines of code to describe a
particular model), and time and space efficiency of inference
algorithms~\cite{WVM14,P16,R17,GXG18}.  However, complex
software systems make integration of probabilistic inference
challenging, and considerations beyond expressiveness
and performance come into play.

\paragraph{Simulation vs. inference} Probabilistic models
often follow a design pattern of simulation-inference: a
significant part of the model is a simulator, running an
algorithm with fixed parameters; the optimal parameters, or
their distribution, are to be inferred. The inferred parameters
are then used by the software system to execute the simulation
independently of inference for forecasting and decision making.

This pattern suggests re-use of the simulator: instead of
implementing the simulator twice, in the probabilistic model and
in the host environment, one can invoke the same code for both
purposes.  However to achieve this, the host language must
coincide with the implementation language of the probabilistic
model, on one hand, and allow a computationally efficient
implementation of the simulation, on the other hand. Some
probabilistic systems (Figaro~\cite{P09},
Anglican~\cite{TMY+16}, Turing~\cite{GXG18}) are built with
tight integration with the host environment in mind; more often
than not though the probabilistic code is not trivial to re-use.

\paragraph{Data interface} The data for inference may come from
a variety of sources: network, databases, distributed file
systems, and in many different formats. Efficient inference
depends on fast data access and updating. Libraries for data
access and manipulation are available in the host environment.
While the host environment can be used as a proxy retrieving and
transforming the data, such as in the case of Stan~\cite{Stan17}
integrations, sometimes direct access from the probabilistic
code is the preferred option, for example when the data is
streamed or retrieved conditionally, as in active
learning~\cite{SLB15}.

A flexible data interface is also beneficial for support of
mini-batch optimization~\cite{LZC+14}. Mini-batch optimization
is used when the data set is too big to fit in memory, and the
objective function gradient is estimated based on mini-batches
--- small portions of data. Different models and inference
objectives may require different mini-batch loading schemes.
For best performance of mini-batch optimization, it should be
possible to program loading of mini-batches on case-by-case
basis.

\paragraph{Integration and deployment} Deployment of software
systems is a delicate process involving automatic builds and
maintenance of dependencies. Adding a component, which possibly
introduces additional software dependencies or even a separate
runtime, complicates deployment. Minimizing the burden of
probabilistic programming on the integration and deployment
process should be a major consideration in design or selection
of probabilistic programming tools. Probabilistic programming
systems that are implemented or provide an interface in a
popular programming language, e.g.  Python
(Edward~\cite{THS+17}, Pyro~\cite{BCJ+19}) are easier to
integrate and deploy, however the smaller the footprint of a
probabilistic system, the easier is the adoption.
An obstacle in integration of probabilistic programming lies
also in adoption of the probabilistic programming
language~\cite{MR12}, if the latter differs from the
implementation language of the rest of the system.

\section{Guidelines}

Based on the experience of developing and deploying solutions
using different probabilistic environments, we came up with
guidelines to implementation of a probabilistic programming
facility for server-side applications. We believe that these
guidelines, when followed, help easier integration of
probabilistic programming inference into large-scale 
software systems.

\begin{enumerate}
\item A probabilistic model should be programmed in the host
programming language. The facility may impose a discipline on
model implementation, such as through interface constraints, but
otherwise supporting unrestricted use of the host language for
implementation of the model.

\item Built-in and user-defined data structures and libraries
should be accessible in the probabilistic programming model.
Inference techniques relying on the code structure, such as
those based on automatic differentiation, should support the
use of common data structures of the host language.

\item The model code should be reusable between inference and
simulation. The code which is not required solely for inference
should be written once for both inference of parameters and use
of the parameters in the host environment.  It should be
possible to run simulation outside the probabilistic model without
runtime or memory overhead imposed by inference needs.
\end{enumerate}

\section{Probabilistic Programming in Go}

In line with the guidelines, we have implemented a probabilistic
programming facility for the Go programming language, Infergo.
%(\url{http://infergo.ml/}). 
We have chosen Go because Go is a
small but expressive programming language with efficient
implementation, which has recently become quite popular for
computation-intensive server-side programming. Infergo is used
in production environment for inference of mission-critical
algorithm parameters.

\subsection{An Overview of Infergo}

A probabilistic model in Infergo is an implementation of an
interface requiring a single method \lstinline{Observe} which
accepts a vector (a Go \textit{slice}) of floats, the parameters
to infer, and returns a single float, interpreted as
unnormalized log-likelihood of the posterior distribution.
Implementation of model methods can be written in virtually
unrestricted Go and use any Go libraries.

For inference, Infergo relies on automatic differentiation. The
source code of the model is translated by a command-line tool
provided by Infergo into an equivalent model with reverse-mode
automatic differentiation of the log-likelihood with respect to
the parameters applied. The differentiation operates on the
built-in floating-point type and incurs only a small
computational overhead. However, even this overhead is avoided
when the model code is executed outside of inference algorithms:
both the original and the differentiated model are
simultaneously available to the rest of the program code, so the
methods can be called on the differentiated model for inference,
and on the original model for the most efficient execution with
the inferred parameters.

The Go programming language and development environment offer
capabilities which made implementation of Infergo affordable.

\begin{itemize}
\item The Go parser and abstract syntax tree serializer are
	a part of the standard library. Parsing, transforming,
	and generating Go source code is straightforward and
	effortless.
\item Type inference (or \textit{type checking} as it is
	called in the Go ecosystem), also provided in the
	standard library, augments parsing and allows to
	selectively apply transformation-based automatic
	differentiation  based on static expression types. 
\item Go compiles and runs fast. Fast compilation and
	execution speeds allow to use the same facility for both
	exploratory design of probabilistic models and for
	inference in production environment.
\item Go offers efficient parallel execution as a
	first-class feature, via so-called \textit{goroutines}.
	Goroutines streamline implementation of sampling-based
	inference algorithms. Sample generators and consumers
	are run in parallel, communicating through channels. 
	Inference is easy to parallelize in order to exploit
	hardware multi-processing, and samples are retrieved
	lazily for postprocessing. 
\end{itemize}

\subsection{A Basic Example}

Let us illustrate the use of Infergo on a basic example serving
as a probabilistic programming equivalent of the "Hello, World!"
program --- inferring parameters of a unidimensional Normal
distribution. In this example, the model object holds the set of
observations from which the distribution parameters are to be
inferred:
\begin{lstlisting}
type Model struct {
    Data []float64
}
\end{lstlisting}

The \lstinline{Observe} method computes the log-likelihood of
the distribution parameters. The Normal distribution has two
parameters: mean $\mu$ and standard deviation $\sigma$. To ease
the inference, positive $\sigma$ is transformed to unrestricted
$\log \sigma$, and the parameter vector \lstinline{x} is $[\mu,
\log \sigma]$. \lstinline{Observe} first imposes a prior on the
parameters, and then conditions the posterior distribution on
the observations:

\begin{lstlisting}
// x[0] is the mean,
// x[1] is the log stddev of the distribution
func (m *Model) Observe(x []float64) float64 {
    // Our prior is a unit normal ...
    ll := Normal.Logps(0, 1, x...)
    // ... but the posterior is based 
	// on data observations.
	ll += Normal.Logps(x[0], math.Exp(x[1]),
	                   m.Data...)
    return ll
}
\end{lstlisting}

For inference, we supply the data and optionally initialize the
parameter vector. Here the data is hard-coded, but in general
can be read from a file, a database, a network connection,
or dynamically generated by another part of the program:

\begin{lstlisting}
// Data
m := &Model{[]float64{
	-0.854, 1.067, -1.220, 0.818, -0.749,
	0.805, 1.443, 1.069, 1.426, 0.308}}

// Parameters
x := []float64{}
\end{lstlisting}
	
The fastest and most straightforward inference is the maximum
\textit{a posteriori} (MAP) estimation, which can be done using
a gradient descent algorithm (Adam~\cite{KB15} in this case).

\begin{lstlisting}
opt := &infer.Adam{
    Rate:  0.01,
}
for iter := 0; iter != 1000; iter++ {
    opt.Step(m, x)
}
\end{lstlisting}

To recover the full posterior distribution of
the parameters we use Hamiltonian Monte Carlo~\cite{N12}:

\begin{lstlisting}
hmc := &infer.HMC{
	Eps: 0.1,
}
samples := make(chan []float64)
hmc.Sample(m, x, samples)
for i := 0; i != 5000; i++ {
	x = <-samples
}
hmc.Stop()
\end{lstlisting}

Models are of course not restricted to linear code, and may
comprise multiple methods containing loops, conditional
statements, function and method calls, and recursion. Case
studies~(Section~\ref{sec:case-studies}) provide elaborated
model examples.

\section{Model Syntax and Rules of Differentiation}

In Infergo, the inference is performed on a model. Just like in
Stan~\cite{Stan17} models, latent random variables in Infergo
models are continuous. Additionally, Infergo can express and
perform inference on non-deterministic models by including
stochastic choices in the model code.

An Infergo model is a data type and methods defined on the data
type.  Inference algorithms accept a model object as an
argument.  A model object usually encapsulates observations ---
the data to which the parameter values are adapted.

A model is implemented in Go, virtually
unrestricted\footnote{There are insignificant implementation
limitations which are gradually lifted as Infergo is being
developed.}. Because of this, the rules of model specification
and differentiation are quite straightforward. Nonetheless, the
differentiation rules ensure that provided that the model's
\lstinline{Observe} method (including calls to other model methods)
is differentiable, the gradient is properly computed~\cite{GW08}.

A model is defined in its own package. The model must
implement interface \lstinline{Model} containing a single method
\lstinline{Observe}. \lstinline{Observe} accepts a slice of
\lstinline{float64} --- the model parameters --- and returns a
\lstinline{float64} scalar. For probabilistic inference, the
returned value is interpreted as the log-likelihood of the model
parameters. Inference models rely on computing the gradient of
the returned value with respect to the model parameters through
automatic differentiation. In the model's source code:

\begin{enumerate}
	\item Methods on the type implementing \lstinline{Model} returning a
		single \lstinline{float64} or nothing are differentiated.
	\item Within the methods, the following is differentiated:
		\begin{itemize}
			\item assignments to \lstinline{float64} (including parallel
				assignments if all values are of type
				\lstinline{float64});
			\item returns of \lstinline{float64};
			\item standalone calls to methods on the type implementing
				\lstinline{Model} (apparently called for side  effects on
				the model).
		\end{itemize}
\end{enumerate}

Functions for which the gradient is provided rather than derived
through automatic differentiation are called
\textit{elementals}~\cite{GW08}.  Derivatives do not propagate
through a function that is not an elemental or a call to a model
method. If a derivative is not registered for an elemental,
calling the elemental in a differentiated context will cause a
run-time error.

Functions are considered elementals (and must have a
registered derivative) if their signature is of kind
\lstinline{func (float64, float64*) float64}; that is, 
one or more non-variadic \lstinline{float64} arguments and
\lstinline{float64} return value. For example, function
\lstinline{func (float64, float64, float64) float64}
is considered elemental, while functions
\begin{itemize}
	\item \lstinline{func (...float64) float64}
	\item \lstinline{func ([]float64) float64}
	\item \lstinline{func (int, float64) float64}
\end{itemize}
are not. Gradients for selected functions from the
\lstinline{math} package are pre-defined (\lstinline{Sqrt},
\lstinline{Exp}, \lstinline{Log}, \lstinline{Pow},
\lstinline{Sin}, \lstinline{Cos}, \lstinline{Tan}). Auxiliary
elemental functions with pre-defined gradients are provided in a
separate package.

Any user-defined function of appropriate kind can be used as an
elemental inside model methods. An elemental must be declared
for the function using a call to
\begin{lstlisting}
func RegisterElemental(
	f interface{}, 
	g func(value float64,
		   params ...float64) []float64)
\end{lstlisting} For
example, the call
\begin{lstlisting}
RegisterElemental(Sigm,
	func(value float64,
	     _ ...float64) []float64 {
		return []float64{value*(1-value)}
	})
\end{lstlisting}
registers the gradient for function \lstinline{Sigm}.

\section{Inference}

Out of the box, Infergo offers 
\begin{itemize}
	\item maximum \textit{a posteriori} (MAP) estimation by stochastic
		gradient descent~\cite{R16}; 
	\item full posterior inference by Hamiltonian Monte
		Carlo~\cite{N12,HG11}.
\end{itemize}

When only a point estimate of the model parameters is required,
as, for example, in the simulation-inference case, stochastic
gradient descent is a fast and robust inference family. Infergo
offers stochastic gradient descent with momentum and
Adam~\cite{KB15}.  The inference is performed by calling the
\lstinline{Step} method of the optimizer until a termination
condition is met:
\begin{lstlisting}
for !<termination condition> {
    optimizer.Step(model, parameters)
}
\end{lstlisting}

When the full posterior must be recovered for integration,
Hamiltonian Monte Carlo (HMC) can be used instead. Vanilla HMC
as well as the No-U-Turn sampler (NUTS)~\cite{HG11} are
provided. To support lazy any-time computation, the inference
runs in a separate \textit{goroutine}, connected by a channel to
the caller, and asynchronously writes samples to the channel.
The caller reads as many samples from the channel as needed, and
further postprocesses them:
\begin{lstlisting}
samples := make(chan []float64)
hmc.Sample(model, parameters, samples)
for !<termination condition> {
	sample = <-samples
	// postprocess the sample
}
hmc.Stop()
\end{lstlisting}

Other inference schemes, most notably automatic differentiation
variational inference~\cite{KTR+17}, are planned for addition
in future versions of Infergo. However, an inference algorithm
does not have to be a part of Infergo to work with Infergo
models. Since Infergo models operate on a built-in Go floating
point type, \lstinline{float64}, any third-party inference or
optimization library can be used as well. 

As an example, the Gonum~\cite{Gonum} library for numerical
computations contains an optimization package offering several
algorithms for gradient-based optimization. A Gonum optimization
algorithm requires the function to optimize, as well as the
function's gradient, as parameters. An Infergo model can be
trivially wrapped for use with Gonum, and Infergo provides a
convenience function
\begin{lstlisting}
func FuncGrad(m Model) (
	Func func(x []float64) float64,
	Grad func(grad []float64, x []float64)
		[]float64)
\end{lstlisting}
which accepts a model and returns the model's
\lstinline{Observe} method and its gradient suitable for passing
to Gonum optimization algorithms. Among available algorithms are
advanced versions of stochastic gradient descent, as well as
algorithms of BFGS family, in particular LBFGS~\cite{LN89},
providing for a more efficient MAP estimation than stochastic
gradient descent at the cost of higher memory consumption:
\begin{lstlisting}
function, gradient := FuncGrad(model)
problem := gonum.Problem{Func: function, 
                         Grad: gradient}
result, err := gonum.Minimize(problem, parameters)
\end{lstlisting}

Other third-party implementations of optimization algorithms can
be used just as easily, thanks to Infergo models being pure Go
code operating on built-in Go data types.

\section{Design and Implementation Highlights}

\subsection{Choice of Programming Language}

\subsection{Automatic Differentiation}

\subsection{Composability of Models}

\subsection{Stochasticity and Streaming}

\section{Case Studies}
\label{sec:case-studies}

\subsection{8 schools}

\begin{figure*}
	\begin{minipage}{0.45\textwidth}
	\begin{lstlisting}[language=Stan,framexleftmargin=10pt,numbers=left]
data {
  int<lower=0> J;
  vector[J] y;
  vector<lower=0>[J] sigma;
}

parameters {
  real mu;
  real<lower=0> tau;
  vector[J] eta;
}

transformed parameters {
  vector[J] theta;
  theta = mu + tau * eta;
}

model {
  eta ~ normal(0, 1);
  y ~ normal(theta, sigma);
}
\end{lstlisting}

\centering
a. Stan
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}
\begin{lstlisting}[framexleftmargin=10pt,numbers=left]
type Model struct {
  J          int
  Y          []float64
  Sigma      []float64
}


func (m *Model) Observe(x []float64) float64 {
  mu := x[0]
  tau := math.Exp(x[1])
  eta := x[2:]

  ll := Normal.Logp(0, 1, eta)

  for i, y := range m.Y {
    theta := mu + tau*eta[i]
    ll += Normal.Logp(theta, m.Sigma[i], y)
  }

  return ll
}
\end{lstlisting}
\centering
b. Infergo
	\end{minipage}
	\caption{8 schools: Stan vs. Infergo. The Go implementation
	has a similar length and structure to the Stan model.}
	\label{fig:8-schools}
\end{figure*}

\subsection{Linear regression}

\begin{figure}
\begin{lstlisting}[framexleftmargin=10pt,numbers=left]
type Model struct {
  Data  [][]float64
}

func (m *Model) Observe(x []float64)
    float64 {
  ll := 0.

  alpha, beta := x[0], x[1]
  sigma := math.Exp(x[2])

  for i := range m.Data {
    ll += Normal.Logp(
      m.Simulate(m.Data[i][0], alpha, beta),
      sigma, m.Data[i][1])
  }
  return ll

// Simulate predicts y for x based
// on inferred parameters.
func (m *Model) Simulate(x, alpha, beta float64)
    float64 {
  y := alpha +beta*x
  return y
}
\end{lstlisting}
\caption{Linear regression: the simulator is re-used in both
	inference and prediction.}
\label{fig:linear-regression}
\end{figure}

\subsection{Gaussian mixture}

\begin{figure}
\begin{lstlisting}[framexleftmargin=10pt,numbers=left]
type Model struct {
  Data  []float64 // samples
  NComp int       // number of components
}

func (m *Model) Observe(x []float64) float64 {
  ll := 0.0
  mu := make([]float64, m.NComp)
  sigma := make([]float64, m.NComp)

  // Fetch component parameters
  for j := 0; j != m.NComp; j++ {
    mu[j] = x[2*j]
    sigma[j] = math.Exp(x[2*j+1])
  }

  // Compute log likelihood of mixture
  // given the data
  for i := 0; i != len(m.Data); i++ {
    var l float64
    for j := 0; j != m.NComp; j++ {
      lj := Normal.Logp(mu[j], sigma[j], m.Data[i])
      if j == 0 {
        l = lj
      } else {
        l = mathx.LogSumExp(l, lj)
      }
    }
    ll += l
  }
  return ll
}
\end{lstlisting}
\caption{Gaussian mixture: Infergo can handle models with
	dynamic control structure.}
\label{fig:gaussian-mixture}
\end{figure}


\subsection{Performance}

Table~\ref{tab:memory-runtime} provides memory consumption and
running time measurements on basic models to illustrate
Infergo's performance.  The measurements were obtained on a
2.3GHz Intel Core 5 CPU with 8GB of memory for 1000 iterations
of Hamiltonian Monte Carlo with 10 leapfrog steps. Note that
log-likelihood computation for standard distributions is not
optimized yet. Quite the opposite: since models in Infergo are
fully composable, primitive distributions are themselves
implemented as Infergo models and automatically differentiated.

{\smaller
\begin{table}[H]
\caption{Memory and running times for 1000 iterations
of HMC with 10 leapfrog steps.}
\label{tab:memory-runtime}
\begin{tabular}{r | c |  c | c}
	{\it model}  & \multicolumn{2}{c|}{\it time} & {\it memory} \\ 
	 & {\it compilation} & {\it execution} & \\\hline
	8 schools & 0.15s & 0.6s & 5.5MB \\
	10D normal, 100 points & 0.15s & 3.0s & 5.7MB \\
	50D normal, 100 points & 0.15s & 13.0s & 5.8MB 
\end{tabular}
\end{table}}

\section{Conclusion}

Building a production software system which employs
probabilistic inference as an integral part of prediction and
decision making algorithms is challenging. To address the
challenges, we proposed guidelines for implementation of a
deployable probabilistic programming facility. Further on, we
introduced Infergo, a probabilistic programming facility in Go,
as a reference implementation in accordance with the guidelines.
Infergo allows to program inference models in virtually
unrestricted Go, and to perform inference on the model using
either supplied or third-party algorithms. We demonstrated on
case studies how Infergo overcomes the challenges by following
the guidelines.

A probabilistic programming facility similar to Infergo can be
added to most modern general-purpose programming languages, in
particular, those used for implementation of large-scale software
systems, making probabilistic programming inference more
accessible in large scale server-side applications. We described
Infergo design choices and implementation techniques in intent
to encourage broader adoption of probabilistic programming in
production software systems and ease implementation of similar
facilities in other languages and software environments.

Infergo is an ongoing project. Development of Infergo is
influenced both by feedback from the open-source Go community
and by the needs of the software system in which Infergo was
initially deployed, and which still supplies the most demanding
use cases in terms of flexibility, scalability, and ease of
integration and maintenance. Deploying probabilistic programming
as a part of production software system has a mutual benefit of
both improving the core algorithms of the system and of helping
to shape and improve probabilistic modeling and inference, thus
further advancing the field of probabilistic programming.

\end{sloppypar}


\clearpage
\clearpage

\bibliography{refs}

\end{document}
